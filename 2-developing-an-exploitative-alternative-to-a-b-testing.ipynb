{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Developing an exploitative alternative to A/B testing\n\nThis is the code accompaning my [blog post](https://medium.com/@Lando-L/beyond-the-basics-reinforcement-learning-with-jax-part-ii-developing-an-exploitative-9423cb6b2fa5) on multi-arm bandits.","metadata":{}},{"cell_type":"markdown","source":"## Implementing the environment","metadata":{}},{"cell_type":"code","source":"import jax\n\n# Numpy API with hardware acceleration and automatic differentiation\nfrom jax import numpy as jnp\n\n# Low level operators\nfrom jax import lax\n\n# API for working with pseudorandom number generators\nfrom jax import random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-11T10:37:14.417181Z","iopub.execute_input":"2023-05-11T10:37:14.419683Z","iopub.status.idle":"2023-05-11T10:37:16.309290Z","shell.execute_reply.started":"2023-05-11T10:37:14.419576Z","shell.execute_reply":"2023-05-11T10:37:16.307943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random seed to make our experiment replicable \nSEED = 42\n\n# Number of visitors we want to simulate\nNUM_VISITS = 10000\n\n# Expected click rates for the five variants with the\nCLICK_RATES = [0.042, 0.03, 0.035, 0.038, 0.045]","metadata":{"execution":{"iopub.status.busy":"2023-05-11T10:37:16.311415Z","iopub.execute_input":"2023-05-11T10:37:16.313346Z","iopub.status.idle":"2023-05-11T10:37:16.320566Z","shell.execute_reply.started":"2023-05-11T10:37:16.313279Z","shell.execute_reply":"2023-05-11T10:37:16.319398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visit(state, timestep, click_rates, policy_fn, update_fn):\n    \"\"\"\n    Simulates a user visit.\n    \"\"\"\n\n    # Unpacking the experiment state into\n    # the agent's parameters and the random number generator\n    params, rng = state\n\n    # Splitting the random number generator\n    next_rng, policy_rng, user_rng = random.split(rng, num=3)\n\n    # Selecting the variant to show the user, based on\n    # the given policy, the agent's paramters, and the current timestep\n    variant = policy_fn(params, timestep, policy_rng)\n\n    # Randomly simulating the user click, based on\n    # the variant's click rate\n    clicked = random.uniform(user_rng) < click_rates[variant]\n\n    # Calculating the agent's updated parameters, based on\n    # the current parameters, the selected variant,\n    # and whether or not the user clicked\n    next_params = update_fn(params, variant, clicked)\n    \n    # Returning the updated experiment state (params and rng) and\n    # whether or not the user clicked\n    return (next_params, next_rng), clicked","metadata":{"execution":{"iopub.status.busy":"2023-05-11T10:37:16.322835Z","iopub.execute_input":"2023-05-11T10:37:16.324297Z","iopub.status.idle":"2023-05-11T10:37:16.342707Z","shell.execute_reply.started":"2023-05-11T10:37:16.324227Z","shell.execute_reply":"2023-05-11T10:37:16.341482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Implementing the policies","metadata":{}},{"cell_type":"code","source":"def action_value_init(num_variants):\n    \"\"\"\n    Returns the initial action values\n    \"\"\"\n\n    return {\n        'n': jnp.ones(num_variants, dtype=jnp.int32),\n        'q': jnp.ones(num_variants, dtype=jnp.float32)\n    }\n\ndef action_value_update(params, variant, clicked):\n    \"\"\"\n    Calculates the updated action values\n    \"\"\"\n\n    # Reading n and q parameters of the selected variant\n    n, q = params['n'][variant], params['q'][variant]\n\n    # Converting the boolean clicked variable to a float value\n    r = clicked.astype(jnp.float32)\n\n    return {\n        # Incrementing the counter of the taken action by one\n        'n': params['n'].at[variant].add(1),\n\n        # Incrementally updating the action-value estimate\n        'q': params['q'].at[variant].add((r - q) / n)\n    }","metadata":{"execution":{"iopub.status.busy":"2023-05-11T10:42:34.894424Z","iopub.execute_input":"2023-05-11T10:42:34.894951Z","iopub.status.idle":"2023-05-11T10:42:34.907444Z","shell.execute_reply.started":"2023-05-11T10:42:34.894907Z","shell.execute_reply":"2023-05-11T10:42:34.905438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def epsilon_greedy_policy(params, timestep, rng, epsilon):\n    \"\"\"\n    Randomly selects either the variant with highest action-value,\n    or an arbitrary variant.\n    \"\"\"\n\n    # Selecting a random variant\n    def explore(q, rng):\n        return random.choice(rng, jnp.arange(len(q)))\n    \n    # Selecting the variant with the highest action-value estimate\n    def exploit(q, rng):\n        return jnp.argmax(q)\n    \n    # Splitting the random number generator \n    uniform_rng, choice_rng = random.split(rng)\n    \n    # Deciding randomly whether to explore or to exploit\n    return lax.cond(\n        random.uniform(uniform_rng) < epsilon,\n        explore,\n        exploit,\n        params['q'],\n        choice_rng\n    )","metadata":{"execution":{"iopub.status.busy":"2023-05-11T10:38:22.536683Z","iopub.execute_input":"2023-05-11T10:38:22.537290Z","iopub.status.idle":"2023-05-11T10:38:22.548254Z","shell.execute_reply.started":"2023-05-11T10:38:22.537233Z","shell.execute_reply":"2023-05-11T10:38:22.546690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def boltzmann_policy(params, timestep, rng, tau):\n    \"\"\"\n    Randomly selects a variant proportional to the current action-values\n    \"\"\"\n\n    return random.choice(\n        rng,\n        jnp.arange(len(params['q'])),\n        # Turning the action-value estimates into a probability distribution\n        # by applying the softmax function controlled by tau\n        p=jax.nn.softmax(params['q'] / tau)\n    )","metadata":{"execution":{"iopub.status.busy":"2023-05-11T10:38:26.026602Z","iopub.execute_input":"2023-05-11T10:38:26.027582Z","iopub.status.idle":"2023-05-11T10:38:26.036858Z","shell.execute_reply.started":"2023-05-11T10:38:26.027527Z","shell.execute_reply":"2023-05-11T10:38:26.034721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def upper_confidence_bound_policy(params, timestep, rng, confidence):\n    \"\"\"\n    Selects the variant with highest action-value plus upper confidence bound\n    \"\"\"\n\n    # Read n and q parameters\n    n, q = params['n'], params['q']\n\n    # Calculating each variant's upper confidence bound\n    # and selecting the variant with the highest value\n    return jnp.argmax(q + confidence * jnp.sqrt(jnp.log(timestep) / n))","metadata":{"execution":{"iopub.status.busy":"2023-05-11T10:38:28.989179Z","iopub.execute_input":"2023-05-11T10:38:28.990770Z","iopub.status.idle":"2023-05-11T10:38:28.999289Z","shell.execute_reply.started":"2023-05-11T10:38:28.990710Z","shell.execute_reply":"2023-05-11T10:38:28.996724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def beta_init(num_variants):\n    \"\"\"\n    Returns the initial hyperparameters of the beta distribution\n    \"\"\"\n\n    return {\n        'a': jnp.ones(num_variants, dtype=jnp.int32),\n        'b': jnp.ones(num_variants, dtype=jnp.int32)\n    }\n\ndef beta_update(params, variant, clicked):\n    \"\"\"\n    Calculates the updated hyperparameters of the beta distribution\n    \"\"\"\n\n    # Incrementing alpha by one\n    def increment_alpha(a, b):\n        return {'a': a.at[variant].add(1), 'b': b}\n    \n    # Incrementing beta by one\n    def increment_beta(a, b):\n        return {'b': b.at[variant].add(1), 'a': a}\n    \n    # Incrementing either alpha or beta\n    # depending on whether or not the user clicked\n    return lax.cond(\n        clicked,\n        increment_alpha,\n        increment_beta,\n        params['a'],\n        params['b']\n    )","metadata":{"execution":{"iopub.status.busy":"2023-05-11T10:42:42.188551Z","iopub.execute_input":"2023-05-11T10:42:42.189171Z","iopub.status.idle":"2023-05-11T10:42:42.201311Z","shell.execute_reply.started":"2023-05-11T10:42:42.189111Z","shell.execute_reply":"2023-05-11T10:42:42.199322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def thompson_policy(params, timestep, rng):\n    \"\"\"\n    Randomly sampling click rates for all variants\n    and selecting the variant with the highest sample\n    \"\"\"\n\n    return jnp.argmax(random.beta(rng, params['a'], params['b']))","metadata":{"execution":{"iopub.status.busy":"2023-05-11T10:42:44.117377Z","iopub.execute_input":"2023-05-11T10:42:44.117884Z","iopub.status.idle":"2023-05-11T10:42:44.126134Z","shell.execute_reply.started":"2023-05-11T10:42:44.117841Z","shell.execute_reply":"2023-05-11T10:42:44.124541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Implementing the evaluation","metadata":{}},{"cell_type":"code","source":"from functools import partial\nfrom matplotlib import pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-05-11T10:39:03.572337Z","iopub.execute_input":"2023-05-11T10:39:03.572977Z","iopub.status.idle":"2023-05-11T10:39:03.580966Z","shell.execute_reply.started":"2023-05-11T10:39:03.572927Z","shell.execute_reply":"2023-05-11T10:39:03.578843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(policy_fn, init_fn, update_fn):\n    \"\"\"\n    Simulating the experiment for NUM_VISITS users\n    while accumulating the click history\n    \"\"\"\n\n    return lax.scan(\n        # Compiling the visit function using just-in-time (JIT) compilation\n        # for better performance\n        jax.jit(\n            # Partially applying the visit function by fixing\n            # the click_rates, policy_fn, and update_fn parameters \n            partial(\n                visit,\n                click_rates=jnp.array(CLICK_RATES),\n                policy_fn=jax.jit(policy_fn),\n                update_fn=jax.jit(update_fn)\n            )\n        ),\n        \n        # Initialising the experiment state using\n        # init_fn and a new PRNG key\n        (init_fn(len(CLICK_RATES)), random.PRNGKey(SEED)),\n        \n        # Setting the number steps of the experiment\n        jnp.arange(1, NUM_VISITS + 1)\n    )","metadata":{"execution":{"iopub.status.busy":"2023-05-11T10:42:46.472633Z","iopub.execute_input":"2023-05-11T10:42:46.474332Z","iopub.status.idle":"2023-05-11T10:42:46.485534Z","shell.execute_reply.started":"2023-05-11T10:42:46.474269Z","shell.execute_reply":"2023-05-11T10:42:46.483925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def regret(history):\n    \"\"\"\n    Calculates the regret for every action in the experiment history\n    \"\"\"\n\n    # Calculating regret with regard to picking the optimal (0.045) variant\n    def fn(acc, reward):\n        n, v = acc[0] + 1, acc[1] + reward\n        return (n, v), 0.045 - (v / n)\n    \n    # Calculating regret values over entire history\n    _, result = lax.scan(\n        jax.jit(fn),\n        (jnp.array(0), jnp.array(0)),\n        history\n    )\n    \n    return result","metadata":{"execution":{"iopub.status.busy":"2023-05-11T10:42:51.272323Z","iopub.execute_input":"2023-05-11T10:42:51.272872Z","iopub.status.idle":"2023-05-11T10:42:51.283516Z","shell.execute_reply.started":"2023-05-11T10:42:51.272825Z","shell.execute_reply":"2023-05-11T10:42:51.281512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Epsilon greedy policy\n(epsilon_greedy_params, _), epsilon_greedy_history = evaluate(\n    policy_fn=partial(epsilon_greedy_policy, epsilon=0.1),\n    init_fn=action_value_init,\n    update_fn=action_value_update\n)\n\n# Boltzmann policy\n(boltzmann_params, _), boltzmann_history = evaluate(\n    policy_fn=partial(boltzmann_policy, tau=1.0),\n    init_fn=action_value_init,\n    update_fn=action_value_update\n)\n\n# Upper confidence bound policy\n(ucb_params, _), ucb_history = evaluate(\n    policy_fn=partial(upper_confidence_bound_policy, confidence=2),\n    init_fn=action_value_init,\n    update_fn=action_value_update\n)\n\n# Thompson sampling policy\n(ts_params, _), ts_history = evaluate(\n    policy_fn=thompson_policy,\n    init_fn=beta_init,\n    update_fn=beta_update\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T10:42:53.101462Z","iopub.execute_input":"2023-05-11T10:42:53.102000Z","iopub.status.idle":"2023-05-11T10:42:59.150191Z","shell.execute_reply.started":"2023-05-11T10:42:53.101958Z","shell.execute_reply":"2023-05-11T10:42:59.148755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualisation\nfig, ax = plt.subplots(figsize=(16, 8))\n\nx = jnp.arange(1, NUM_VISITS + 1)\n\nax.set_xlabel('Number of visits')\nax.set_ylabel('Regret')\n\nax.plot(x, jnp.repeat(jnp.mean(jnp.array(CLICK_RATES)), NUM_VISITS), label='A/B Testing')\nax.plot(x, regret(epsilon_greedy_history), label='Espilon Greedy Policy')\nax.plot(x, regret(boltzmann_history), label='Boltzmann Policy')\nax.plot(x, regret(ucb_history), label='UCB Policy')\nax.plot(x, regret(ts_history), label='TS Policy')\n\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-11T10:44:03.024351Z","iopub.execute_input":"2023-05-11T10:44:03.024867Z","iopub.status.idle":"2023-05-11T10:44:05.298416Z","shell.execute_reply.started":"2023-05-11T10:44:03.024823Z","shell.execute_reply":"2023-05-11T10:44:05.296471Z"},"trusted":true},"execution_count":null,"outputs":[]}]}